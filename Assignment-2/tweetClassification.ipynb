{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of tweetClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "F2j6YG0Iqa1l",
        "e3mYtxxahlt_",
        "4o90LUN4hxNm",
        "gAopzrYR3o5L",
        "aR2ltvsl5gtx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nd20104/CE888/blob/main/Assignment-2/tweetClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2j6YG0Iqa1l"
      },
      "source": [
        "# Import important libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgCsTBI_3Yx5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOl5MeN-t_Ts"
      },
      "source": [
        "import seaborn as sns\n",
        "import re\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_extraction.text import  CountVectorizer, TfidfVectorizer\n",
        "from sklearn import svm\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfYR5QMRUXgR"
      },
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPhrtQdECH4W"
      },
      "source": [
        "My chosen datasets are: **Offensive language identification, Hate speech detection and Emotion recongnition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtLBsPu1g5SD"
      },
      "source": [
        "## Loading *Emotion datasets*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfjXPCshbaWi"
      },
      "source": [
        "\n",
        "#traindf=pd.read_csv('../input/semeval-2018-task-ec/2018-E-c-En-train.txt',encoding='utf-8',sep=\"\\t\")\n",
        "emo_train_text = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/train_text.txt\",encoding='utf-8',sep=\"\\t\", header=None)\n",
        "emo_train_text.columns= [\"tweet_text\"]\n",
        "emo_train_label = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/train_labels.txt\",encoding='utf-8',sep=\"\\t\", header=None)\n",
        "emo_train_label.columns= [\"tweet_label\"]\n",
        "\n",
        "\n",
        "emo_val_text = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/val_text.txt\",encoding='utf-8',sep=\"\\t\", header=None)\n",
        "emo_val_text.columns= [\"tweet_text\"]\n",
        "emo_val_label = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/val_labels.txt\",encoding='utf-8',sep=\"\\t\", header=None)\n",
        "emo_val_label.columns= [\"tweet_label\"]\n",
        "\n",
        "emo_test_text = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/test_text.txt\",encoding='utf-8',sep=\"\\t\", header=None)\n",
        "emo_test_text.columns= [\"tweet_text\"]\n",
        "emo_test_label = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/test_labels.txt\",encoding='utf-8',sep=\"\\t\", header=None)\n",
        "emo_test_label.columns= [\"tweet_label\"]\n",
        "\n",
        "emo_map = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/mapping.txt\",encoding='utf-8',sep=\"\\t\", header=None)\n",
        "emo_map.columns= [\"tweet_label\",\"emotion\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3mYtxxahlt_"
      },
      "source": [
        "## Loading *Hate datasets*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG4xwHGPVm69"
      },
      "source": [
        "#loading Hate speech data\n",
        "hate_train_text = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/train_text.txt\", sep=\"\\t\", quoting=csv.QUOTE_NONE, skip_blank_lines=False, header=None)\n",
        "hate_train_text.columns= [\"tweet_text\"]\n",
        "hate_train_label = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/train_labels.txt\",encoding='utf-8',sep=\"\\n\", header=None)\n",
        "hate_train_label.columns= [\"tweet_label\"]\n",
        "\n",
        "\n",
        "hate_val_text = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/val_text.txt\",sep=\"\\t\", quoting=csv.QUOTE_NONE, skip_blank_lines=False, header=None)\n",
        "hate_val_text.columns= [\"tweet_text\"]\n",
        "hate_val_label = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/val_labels.txt\",encoding='utf-8',sep=\"\\t\", header=None)\n",
        "hate_val_label.columns= [\"tweet_label\"]\n",
        "\n",
        "hate_test_text = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/test_text.txt\",sep=\"\\t\", quoting=csv.QUOTE_NONE, skip_blank_lines=False, header=None)\n",
        "hate_test_text.columns= [\"tweet_text\"]\n",
        "hate_test_label = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/test_labels.txt\",encoding='utf-8',sep=\"\\t\", header=None)\n",
        "hate_test_label.columns= [\"tweet_label\"]\n",
        "\n",
        "hate_map = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/hate/mapping.txt\",encoding='utf-8',sep=\"\\t\", header=None)\n",
        "hate_map.columns= [\"tweet_label\",\"isHate\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o90LUN4hxNm"
      },
      "source": [
        "## Loading *Offensive datasets*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twUdZI1JFKHg"
      },
      "source": [
        "#loading offensive data\n",
        "off_train_text = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/train_text.txt\",encoding='utf-8',sep=\"\\t\", header=None)\n",
        "off_train_text.columns= [\"tweet_text\"]\n",
        "off_train_label = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/train_labels.txt\",encoding='utf-8',sep=\"\\t\", header=None)\n",
        "off_train_label.columns= [\"tweet_label\"]\n",
        "\n",
        "\n",
        "off_val_text = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/val_text.txt\",encoding='utf-8',sep=\"\\t\", header=None)\n",
        "off_val_text.columns= [\"tweet_text\"]\n",
        "off_val_label = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/val_labels.txt\",encoding='utf-8',sep=\"\\t\", header=None)\n",
        "off_val_label.columns= [\"tweet_label\"]\n",
        "\n",
        "off_test_text = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/test_text.txt\",encoding='utf-8',sep=\"\\t\", header=None)\n",
        "off_test_text.columns= [\"tweet_text\"]\n",
        "off_test_label = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/test_labels.txt\",encoding='utf-8',sep=\"\\t\", header=None)\n",
        "off_test_label.columns= [\"tweet_label\"]\n",
        "\n",
        "\n",
        "off_map = pd.read_csv(\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/mapping.txt\",encoding='utf-8',sep=\"\\t\", header=None)\n",
        "off_map.columns= [\"tweet_label\",\"offensive\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyOePmDUivWo"
      },
      "source": [
        "# Cleaning methods used by all datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv-H6ItIgGwj"
      },
      "source": [
        "def clean_tweet(text):\n",
        "  # \"- Removing HTML tags\n",
        "  #   - Removing punctuation\n",
        "  #   - Lowering text\n",
        "  #   \"\n",
        "  # remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    \n",
        "    # convert text to lowercase\n",
        "    text = text.strip().lower()\n",
        "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "    translate_dict = dict((c, \" \") for c in filters)\n",
        "    translate_map = str.maketrans(translate_dict)\n",
        "    text = text.translate(translate_map)\n",
        "    \n",
        "    return text\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSc8ARnaAdCr"
      },
      "source": [
        "def preprocess(text):\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    return \" \".join(new_text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yq52JIXZEOM8"
      },
      "source": [
        "\n",
        "def get_top_n_words(corpus, n=None):\n",
        "    \"\"\"\n",
        "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
        "    \"\"\"\n",
        "    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWMtLJ533CHq"
      },
      "source": [
        "# **Emotion** dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knOXbuzyiO_-"
      },
      "source": [
        "## *Preprocessing*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DM5IBrxuniN0"
      },
      "source": [
        "#concatenating dataframe by columns to make one dataset each for train, validation and test df\n",
        "df_emo_train = pd.concat([emo_train_text,emo_train_label], axis=1)\n",
        "df_emo_val = pd.concat([emo_val_text,emo_val_label], axis=1)\n",
        "df_emo_test = pd.concat([emo_test_text,emo_test_label], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HgpVrTzpHhD"
      },
      "source": [
        "#joining mapping dataset with train, validation and test datasets\n",
        "emo_combo_train = pd.merge(df_emo_train,emo_map, on='tweet_label')\n",
        "emo_combo_val = pd.merge(df_emo_val,emo_map, on='tweet_label')\n",
        "emo_combo_test = pd.merge(df_emo_test,emo_map, on='tweet_label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWr-8oDdiA5b"
      },
      "source": [
        "# concatenating train and validation datasets by row to make one complete training data\n",
        "emo_train_val = pd.concat([emo_combo_train,emo_combo_val])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcitYJD5UmGH"
      },
      "source": [
        "train_emo = emo_train_val.copy()\n",
        "train_emo['text_clean'] = train_emo['tweet_text'].apply(str).apply(lambda x: preprocess(x)) # text\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-8H0sQioOTT"
      },
      "source": [
        "## *Visualization*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h9ZNblVE6Ho"
      },
      "source": [
        "#getting top 20 words in each classification of Emotion\n",
        "top_sad_words = get_top_n_words(emo_sad)\n",
        "top_angry_words = get_top_n_words(emo_anger)\n",
        "top_joy_words = get_top_n_words(emo_joy)\n",
        "top_op_words = get_top_n_words(emo_opt)\n",
        "\n",
        "s1 = [x[0] for x in top_sad_words[:20]]\n",
        "s2 = [x[1] for x in top_sad_words[:20]]\n",
        "\n",
        "a1 = [x[0] for x in top_angry_words[:20]]\n",
        "a2 = [x[1] for x in top_angry_words[:20]]\n",
        "\n",
        "j1 = [x[0] for x in top_joy_words[:20]]\n",
        "j2 = [x[1] for x in top_joy_words[:20]]\n",
        "\n",
        "o1 = [x[0] for x in top_op_words[:20]]\n",
        "o2 = [x[1] for x in top_op_words[:20]]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxxrIklAnuoG"
      },
      "source": [
        "## *Training* the model and testing with macro averaged F1 score as evaluation metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Psk3CxQKVV3e"
      },
      "source": [
        "**cleaning** the data and then using Linear SVM for inital training and testing\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owqg5ZX4ZJoH"
      },
      "source": [
        "Using BOW vectorizer from Python with countvectorizer from scikitlearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEnaQuG7YvkM",
        "outputId": "c1a6d4ab-af6e-42de-dfd7-527b6a2bac64"
      },
      "source": [
        "vectorizer = CountVectorizer(stop_words=\"english\",preprocessor=clean_tweet)\n",
        "\n",
        "training_features = vectorizer.fit_transform(emo_train_val[\"tweet_text\"]) \n",
        "test_features = vectorizer.transform(emo_combo_test[\"tweet_text\"])\n",
        "model = LinearSVC()\n",
        "model.fit(training_features,emo_train_val[\"emotion\"])\n",
        "y_pred = model.predict(test_features)\n",
        "\n",
        "score = f1_score(emo_combo_test[\"emotion\"], y_pred, average='macro')\n",
        "print(\"F1 Score on emotion dataset: {}\".format(round(score*100,2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 Score on emotion dataset: 61.03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u1vHVLV1vBS"
      },
      "source": [
        "Taking n-gram: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS_Q5XWV4z96",
        "outputId": "5d8b8e8b-11ad-4d10-bb1f-38c2154e0633"
      },
      "source": [
        "# modeling with countvectorizer and bigrams\n",
        "vectorizer = CountVectorizer(stop_words=\"english\",\n",
        "                             preprocessor=clean_tweet,\n",
        "                             ngram_range=(1, 2))\n",
        "\n",
        "training_features = vectorizer.fit_transform(emo_train_val[\"tweet_text\"])    \n",
        "test_features = vectorizer.transform(emo_combo_test[\"tweet_text\"])\n",
        "\n",
        "# Training\n",
        "model = LinearSVC()\n",
        "model.fit(training_features, emo_train_val[\"emotion\"])\n",
        "y_pred = model.predict(test_features)\n",
        "\n",
        "# Evaluation\n",
        "score = f1_score(emo_combo_test[\"emotion\"], y_pred,average='macro')\n",
        "\n",
        "print(\"F1 score on the emotion dataset: {:.2f}\".format(score*100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score on the emotion dataset: 60.60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YF7Wcce_vT9"
      },
      "source": [
        "bigram also gave same result. next checking TF-IDF  vectorizer  with bigram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egVudqVJ_-do",
        "outputId": "3b19a0e9-8f14-4ee0-df98-226bda245a62"
      },
      "source": [
        "\n",
        "# modeling with TF-IDF and bigrams\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\",\n",
        "                             preprocessor=clean_tweet,\n",
        "                             ngram_range=(1, 2))\n",
        "\n",
        "training_features = vectorizer.fit_transform(emo_train_val[\"tweet_text\"])    \n",
        "test_features = vectorizer.transform(emo_combo_test[\"tweet_text\"])\n",
        "\n",
        "# Training\n",
        "model = LinearSVC()\n",
        "model.fit(training_features, emo_train_val[\"emotion\"])\n",
        "y_pred = model.predict(test_features)\n",
        "\n",
        "# Evaluation\n",
        "score = f1_score(emo_combo_test[\"emotion\"], y_pred,average='macro')\n",
        "\n",
        "print(\"F1 score on the emotion dataset: {:.2f}\".format(score*100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score on the emotion dataset: 63.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZOSMYhiASxp"
      },
      "source": [
        "NOT  huge difference. Now trying the cleaning process given by tweeteval repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNycjn3dGlLs",
        "outputId": "04c78c05-0bf4-4ea4-8e7d-e3f32a9d84e7"
      },
      "source": [
        "# cleaning with process suggested by tweeteval  and then vectorizing with TF-IDF and bigrams to train emotion dataset\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\",\n",
        "                             preprocessor=preprocess,\n",
        "                             ngram_range=(1, 2))\n",
        "\n",
        "training_features = vectorizer.fit_transform(emo_train_val[\"tweet_text\"])    \n",
        "test_features = vectorizer.transform(emo_combo_test[\"tweet_text\"])\n",
        "\n",
        "# Training\n",
        "model = LinearSVC()\n",
        "model.fit(training_features, emo_train_val[\"emotion\"])\n",
        "y_pred = model.predict(test_features)\n",
        "\n",
        "# Evaluation\n",
        "score = f1_score(emo_combo_test[\"emotion\"], y_pred,average='macro')\n",
        "\n",
        "print(\"F1 score Emotion dataset: {:.2f}\".format(score*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score Emotion dataset: 61.35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyKkPGkZYiv7",
        "outputId": "f6f8ce27-148b-4420-9823-a43a0f763a5a"
      },
      "source": [
        "#cleaning with process suggested by tweeteval  and then vectorizing with countvectorizer with bigrams to train emotion dataset by SVM\n",
        "\n",
        "vectorizer = CountVectorizer(#stop_words=\"english\",\n",
        "                             preprocessor=preprocess,\n",
        "                             ngram_range=(1, 2))\n",
        "\n",
        "training_features = vectorizer.fit_transform(emo_train_val[\"tweet_text\"])    \n",
        "test_features = vectorizer.transform(emo_combo_test[\"tweet_text\"])\n",
        "\n",
        "# Training\n",
        "model = LinearSVC()\n",
        "model.fit(training_features, emo_train_val[\"emotion\"])\n",
        "y_pred = model.predict(test_features)\n",
        "\n",
        "# Evaluation\n",
        "score = f1_score(emo_combo_test[\"emotion\"], y_pred,average='macro')\n",
        "\n",
        "print(\"F1 score on the emotion dataset: {:.2f}\".format(score*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score on the emotion dataset: 60.08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAopzrYR3o5L"
      },
      "source": [
        "# **Hate Speech** dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH0k7PHEuLne"
      },
      "source": [
        "## *Preprocessing*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG8wO-bap_Pw"
      },
      "source": [
        "#concatenating dataframe to make one dataset each for train, validation and test df\n",
        "df_hate_train = pd.concat([hate_train_text,hate_train_label], axis=1)\n",
        "df_hate_val = pd.concat([hate_val_text,hate_val_label], axis=1)\n",
        "df_hate_test = pd.concat([hate_test_text,hate_test_label], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHHpmoHowSyE"
      },
      "source": [
        "hate_combo_train = pd.merge(df_hate_train,hate_map, on='tweet_label')\n",
        "hate_combo_val = pd.merge(df_hate_val,hate_map, on='tweet_label')\n",
        "hate_combo_test = pd.merge(df_hate_test,hate_map, on='tweet_label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABLWlydS-uoW"
      },
      "source": [
        "hate_train_val = pd.concat([hate_combo_train, hate_combo_val])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acfeOq0j_BAJ",
        "outputId": "0f90a9d1-66d4-4436-d41f-2eb0b7773d83"
      },
      "source": [
        "#checking total nan values/ empty rows\n",
        "total =0\n",
        "for n in pd.isna(hate_train_val[\"tweet_text\"]):\n",
        "  if n==True:\n",
        "    total = total+1\n",
        "total"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOvMmiW_04Q8"
      },
      "source": [
        "hate_train_val.dropna(inplace=True) #dropping the empty tweets, total 8, as they were included during the reading part\n",
        "hate_combo_test.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYW8fgcVueXT"
      },
      "source": [
        "train_hate = hate_train_val.copy()\n",
        "train_hate['text_clean'] = train_hate['tweet_text'].apply(str).apply(lambda x: preprocess(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "C9RPovzE-wU7",
        "outputId": "081a7ddb-47e0-4059-9d3d-0736faf69bb7"
      },
      "source": [
        "#removing workds like user and amp in clean tweets\n",
        "train_hate['text_clean'] = train_hate['text_clean'].str.replace('user','')\n",
        "train_hate['text_clean'] = train_hate['text_clean'].str.replace('amp','')\n",
        "train_hate['text_clean'] = train_hate['text_clean'].str.replace('@','')\n",
        "train_hate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>tweet_label</th>\n",
              "      <th>isHate</th>\n",
              "      <th>text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@user nice new signage. Are you not concerned ...</td>\n",
              "      <td>0</td>\n",
              "      <td>not-hate</td>\n",
              "      <td>nice new signage. Are you not concerned by Be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hysterical woman like @user</td>\n",
              "      <td>0</td>\n",
              "      <td>not-hate</td>\n",
              "      <td>Hysterical woman like</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Me flirting- So tell me about your father...</td>\n",
              "      <td>0</td>\n",
              "      <td>not-hate</td>\n",
              "      <td>Me flirting- So tell me about your father...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The Philippine Catholic bishops' work for migr...</td>\n",
              "      <td>0</td>\n",
              "      <td>not-hate</td>\n",
              "      <td>The Philippine Catholic bishops' work for migr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>When cuffin season is finally over</td>\n",
              "      <td>0</td>\n",
              "      <td>not-hate</td>\n",
              "      <td>When cuffin season is finally over</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>shut the fuck up you know youre that bitch, wh...</td>\n",
              "      <td>1</td>\n",
              "      <td>hate</td>\n",
              "      <td>shut the fuck up you know youre that bitch, wh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>Where the fuck is your face scary hoe? Dont be...</td>\n",
              "      <td>1</td>\n",
              "      <td>hate</td>\n",
              "      <td>Where the fuck is your face scary hoe? Dont be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Pass #MeritBased Immigration. Kill #ChainMigra...</td>\n",
              "      <td>1</td>\n",
              "      <td>hate</td>\n",
              "      <td>Pass #MeritBased Immigration. Kill #ChainMigra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>I usually dont hate people but I actually hate...</td>\n",
              "      <td>1</td>\n",
              "      <td>hate</td>\n",
              "      <td>I usually dont hate people but I actually hate...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>Cameron stopped immigrants voting on the EU in...</td>\n",
              "      <td>1</td>\n",
              "      <td>hate</td>\n",
              "      <td>Cameron stopped immigrants voting on the EU in...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9992 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            tweet_text  ...                                         text_clean\n",
              "0    @user nice new signage. Are you not concerned ...  ...   nice new signage. Are you not concerned by Be...\n",
              "1                         Hysterical woman like @user   ...                            Hysterical woman like  \n",
              "2        Me flirting- So tell me about your father...   ...      Me flirting- So tell me about your father... \n",
              "3    The Philippine Catholic bishops' work for migr...  ...  The Philippine Catholic bishops' work for migr...\n",
              "4                  When cuffin season is finally over   ...                When cuffin season is finally over \n",
              "..                                                 ...  ...                                                ...\n",
              "995  shut the fuck up you know youre that bitch, wh...  ...  shut the fuck up you know youre that bitch, wh...\n",
              "996  Where the fuck is your face scary hoe? Dont be...  ...  Where the fuck is your face scary hoe? Dont be...\n",
              "997  Pass #MeritBased Immigration. Kill #ChainMigra...  ...  Pass #MeritBased Immigration. Kill #ChainMigra...\n",
              "998  I usually dont hate people but I actually hate...  ...  I usually dont hate people but I actually hate...\n",
              "999  Cameron stopped immigrants voting on the EU in...  ...  Cameron stopped immigrants voting on the EU in...\n",
              "\n",
              "[9992 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDbu4Ibv_zkF",
        "outputId": "09cfed7d-2ca4-4f25-9d60-1047292b3261"
      },
      "source": [
        "# Hate tweet\n",
        "print(\"Hate Tweet example :\",hate_train_val[hate_train_val['isHate']=='hate']['tweet_text'].values[0])\n",
        "#Not-Hate Tweet \n",
        "print(\"Not Hate Tweet example :\",hate_train_val[hate_train_val['isHate']=='not-hate']['tweet_text'].values[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hate Tweet example : A woman who you fucked multiple times saying yo dick small is a compliment you know u hit that spot 😎 \n",
            "Not Hate Tweet example : @user nice new signage. Are you not concerned by Beatlemania -style hysterical crowds crongregating on you… \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xJdxMF0uyrB"
      },
      "source": [
        "## *Visualization*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcM4SJHm_ChZ"
      },
      "source": [
        "#creating different datasets for different emotions\n",
        "hate_yes = train_hate[train_hate['isHate'] == 'hate']['text_clean']\n",
        "hate_no = train_hate[train_hate['isHate'] == 'not-hate']['text_clean']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXTFTwRfLQCp"
      },
      "source": [
        "#getting top 20 words in each classification of Hate dataset\n",
        "top_hate_words = get_top_n_words(hate_yes)\n",
        "top_nonHate_words = get_top_n_words(hate_no)\n",
        "\n",
        "\n",
        "s1 = [x[0] for x in top_hate_words[:20]]\n",
        "s2 = [x[1] for x in top_hate_words[:20]]\n",
        "\n",
        "a1 = [x[0] for x in top_nonHate_words[:20]]\n",
        "a2 = [x[1] for x in top_nonHate_words[:20]]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faaUBqKewzuT"
      },
      "source": [
        "## *Training* the model and testing with macro averaged F1 score as evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddxIjlDy9aVq",
        "outputId": "e31dfbfd-c407-481b-db99-755bd2124e48"
      },
      "source": [
        "vectorizer = CountVectorizer(stop_words=\"english\",preprocessor=clean_tweet)\n",
        "training_features = vectorizer.fit_transform(hate_train_val[\"tweet_text\"])  # feature extraction of train and validation set\n",
        "test_features = vectorizer.transform(hate_combo_test[\"tweet_text\"]) #feature extraction of test set\n",
        "model = LinearSVC()\n",
        "model.fit(training_features,hate_train_val[\"isHate\"])\n",
        "y_pred = model.predict(test_features)\n",
        "\n",
        "score = f1_score(hate_combo_test[\"isHate\"], y_pred, average='macro')\n",
        "print(\"F1 Score on hate detection dataset: {}\".format(round(score*100,2)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 Score on hate detection dataset: 48.83\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tZVrMZSJpcg",
        "outputId": "9f113f11-8e1f-450e-bcb0-f2dc076d210f"
      },
      "source": [
        "# cleaning with basic clean funtion  and then vectorizing with TF-IDF and bigrams for Hate dataset\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\",\n",
        "                             preprocessor=clean_tweet,\n",
        "                             ngram_range=(1, 2))\n",
        "\n",
        "training_features = vectorizer.fit_transform(hate_train_val[\"tweet_text\"])    \n",
        "test_features = vectorizer.transform(hate_combo_test[\"tweet_text\"])\n",
        "\n",
        "# Training\n",
        "model = LinearSVC()\n",
        "model.fit(training_features, hate_train_val[\"isHate\"])\n",
        "y_pred = model.predict(test_features)\n",
        "\n",
        "# Evaluation\n",
        "score = f1_score(hate_combo_test[\"isHate\"], y_pred,average='macro')\n",
        "\n",
        "print(\"F1 score on the Hate dataset: {:.2f}\".format(score*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score on the Hate dataset: 39.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LtaoaSdKMTR",
        "outputId": "e452b978-6284-4065-c1d3-815f322c0249"
      },
      "source": [
        "# cleaning with proecss suggested by tweeteval  and then vectorizing with TF-IDF and bigrams hate dataset\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\",\n",
        "                             preprocessor=preprocess,\n",
        "                             ngram_range=(1, 2))\n",
        "\n",
        "training_features = vectorizer.fit_transform(hate_train_val[\"tweet_text\"])    \n",
        "test_features = vectorizer.transform(hate_combo_test[\"tweet_text\"])\n",
        "\n",
        "# Training\n",
        "model = LinearSVC()\n",
        "model.fit(training_features, hate_train_val[\"isHate\"])\n",
        "y_pred = model.predict(test_features)\n",
        "\n",
        "# Evaluation\n",
        "score = f1_score(hate_combo_test[\"isHate\"], y_pred,average='macro')\n",
        "\n",
        "print(\"F1 score on the Hate detection dataset: {:.2f}\".format(score*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score on the Hate detection dataset: 41.38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwRypSHe1sQ1",
        "outputId": "a3fb36c2-e246-4092-c041-5bf3e627b5f5"
      },
      "source": [
        "# cleaning with proecss suggested by tweeteval  and then vectorizing with countvectorizer and bigrams hate dataset\n",
        "vectorizer = CountVectorizer(#stop_words=\"english\",\n",
        "                             preprocessor=preprocess,\n",
        "                             ngram_range=(1, 2))\n",
        "\n",
        "training_features = vectorizer.fit_transform(hate_train_val[\"tweet_text\"])    \n",
        "test_features = vectorizer.transform(hate_combo_test[\"tweet_text\"])\n",
        "\n",
        "# Training\n",
        "model = LinearSVC()\n",
        "model.fit(training_features, hate_train_val[\"isHate\"])\n",
        "y_pred = model.predict(test_features)\n",
        "\n",
        "# Evaluation\n",
        "score = f1_score(hate_combo_test[\"isHate\"], y_pred,average='macro')\n",
        "\n",
        "print(\"F1 score on the Hate detection dataset: {:.2f}\".format(score*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score on the Hate detection dataset: 48.65\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTv6xpWxKC-w"
      },
      "source": [
        "Best score is generated with countvectorizer, basic cleaning and wothout bigrams\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR2ltvsl5gtx"
      },
      "source": [
        "# **Offensive** dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D85tVii10gEu"
      },
      "source": [
        "## *Preprocessing*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0mgl8nu5gt7"
      },
      "source": [
        "#concatenating dataframe to make one dataset each for train, validation and test df\n",
        "df_off_train = pd.concat([off_train_text,off_train_label], axis=1)\n",
        "df_off_val = pd.concat([off_val_text,off_val_label], axis=1)\n",
        "df_off_test = pd.concat([off_test_text,off_test_label], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNvyzDH55gt7"
      },
      "source": [
        "off_combo_train = pd.merge(df_off_train,off_map, on='tweet_label')\n",
        "off_combo_val = pd.merge(df_off_val,off_map, on='tweet_label')\n",
        "off_combo_test = pd.merge(df_off_test,off_map, on='tweet_label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdCFpTTi4pok"
      },
      "source": [
        "off_train_val = pd.concat([off_combo_train,off_combo_val])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXVPRdJ100Yc"
      },
      "source": [
        "train_off = off_train_val.copy()\n",
        "train_off['text_clean'] = train_off['tweet_text'].apply(str).apply(lambda x: preprocess(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "1svuceCVAa6M",
        "outputId": "e3e599ec-b79a-4b3d-ecf6-debbc73ba8ce"
      },
      "source": [
        "#removing workds like user and amp in clean tweets\n",
        "train_off['text_clean'] = train_off['text_clean'].str.replace('user','')\n",
        "train_off['text_clean'] = train_off['text_clean'].str.replace('amp','')\n",
        "train_off['text_clean'] = train_off['text_clean'].str.replace('@','')\n",
        "\n",
        "train_off"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>tweet_label</th>\n",
              "      <th>offensive</th>\n",
              "      <th>text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@user Bono... who cares. Soon people will unde...</td>\n",
              "      <td>0</td>\n",
              "      <td>not-offensive</td>\n",
              "      <td>Bono... who cares. Soon people will understan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@user Get him some line help. He is gonna be j...</td>\n",
              "      <td>0</td>\n",
              "      <td>not-offensive</td>\n",
              "      <td>Get him some line help. He is gonna be just f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@user @user She is great. Hi Fiona!</td>\n",
              "      <td>0</td>\n",
              "      <td>not-offensive</td>\n",
              "      <td>She is great. Hi Fiona!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@user @user @user @user @user @user @user @use...</td>\n",
              "      <td>0</td>\n",
              "      <td>not-offensive</td>\n",
              "      <td>This is the VetsResistSquadron\"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@user @user Lol. Except he’s the most successf...</td>\n",
              "      <td>0</td>\n",
              "      <td>not-offensive</td>\n",
              "      <td>Lol. Except he’s the most successful preside...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1319</th>\n",
              "      <td>@user @user @user Weak argument considering  -...</td>\n",
              "      <td>1</td>\n",
              "      <td>offensive</td>\n",
              "      <td>Weak argument considering  -GOP congressmen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1320</th>\n",
              "      <td>@user Stop saying this shit you are not going ...</td>\n",
              "      <td>1</td>\n",
              "      <td>offensive</td>\n",
              "      <td>Stop saying this shit you are not going to do...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1321</th>\n",
              "      <td>@user @user @user @user @user Sorry to break i...</td>\n",
              "      <td>1</td>\n",
              "      <td>offensive</td>\n",
              "      <td>Sorry to break it to you but the god of t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1322</th>\n",
              "      <td>. she. is the most powerful woman on this eart...</td>\n",
              "      <td>1</td>\n",
              "      <td>offensive</td>\n",
              "      <td>. she. is the most powerful woman on this eart...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1323</th>\n",
              "      <td>@user Fuck Alan I’m sorry</td>\n",
              "      <td>1</td>\n",
              "      <td>offensive</td>\n",
              "      <td>Fuck Alan I’m sorry</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13240 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             tweet_text  ...                                         text_clean\n",
              "0     @user Bono... who cares. Soon people will unde...  ...   Bono... who cares. Soon people will understan...\n",
              "1     @user Get him some line help. He is gonna be j...  ...   Get him some line help. He is gonna be just f...\n",
              "2                  @user @user She is great. Hi Fiona!   ...                           She is great. Hi Fiona! \n",
              "3     @user @user @user @user @user @user @user @use...  ...                 This is the VetsResistSquadron\"...\n",
              "4     @user @user Lol. Except he’s the most successf...  ...    Lol. Except he’s the most successful preside...\n",
              "...                                                 ...  ...                                                ...\n",
              "1319  @user @user @user Weak argument considering  -...  ...     Weak argument considering  -GOP congressmen...\n",
              "1320  @user Stop saying this shit you are not going ...  ...   Stop saying this shit you are not going to do...\n",
              "1321  @user @user @user @user @user Sorry to break i...  ...       Sorry to break it to you but the god of t...\n",
              "1322  . she. is the most powerful woman on this eart...  ...  . she. is the most powerful woman on this eart...\n",
              "1323                         @user Fuck Alan I’m sorry   ...                               Fuck Alan I’m sorry \n",
              "\n",
              "[13240 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCPP--N91W4Y"
      },
      "source": [
        "## *Training* the model and testing with macro averaged F1 score as evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yflOAwWP5o3I"
      },
      "source": [
        "vectorizer = CountVectorizer(stop_words=\"english\",preprocessor=clean_tweet)\n",
        "\n",
        "training_features = vectorizer.fit_transform(off_train_val[\"tweet_text\"])  # feature extraction of train and validation set\n",
        "test_features = vectorizer.transform(off_combo_test[\"tweet_text\"]) #feature extraction of test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOZv8vD_5sr7",
        "outputId": "faaec08f-9feb-40f2-e844-26d84e45bcea"
      },
      "source": [
        "\n",
        "model = LinearSVC()\n",
        "model.fit(training_features,off_train_val[\"offensive\"])\n",
        "y_pred = model.predict(test_features)\n",
        "\n",
        "score = f1_score(off_combo_test[\"offensive\"], y_pred, average='macro')\n",
        "print(\"F1 Score on offensive language dataset: {}\".format(round(score*100,2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 Score on offensive language dataset: 71.48\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VWHVyTDIFoV",
        "outputId": "51bb8b02-9615-4543-c6be-f4eb8712cd28"
      },
      "source": [
        "# cleaning with basic clean funtion  and then vectorizing with TF-IDF and bigrams for Offensive dataset\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\",\n",
        "                             preprocessor=clean_tweet,\n",
        "                             ngram_range=(1, 2))\n",
        "\n",
        "training_features = vectorizer.fit_transform(off_train_val[\"tweet_text\"])    \n",
        "test_features = vectorizer.transform(off_combo_test[\"tweet_text\"])\n",
        "\n",
        "# Training\n",
        "model = LinearSVC()\n",
        "model.fit(training_features, off_train_val[\"offensive\"])\n",
        "y_pred = model.predict(test_features)\n",
        "\n",
        "# Evaluation \n",
        "score = f1_score(off_combo_test[\"offensive\"], y_pred,average='macro')\n",
        "\n",
        "print(\"F1 score on the offensive language dataset: {:.2f}\".format(score*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score on the offensive language dataset: 74.42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nme82t952ZqW",
        "outputId": "3ef33b89-4f4f-4c27-c172-d82e880b6e64"
      },
      "source": [
        "# cleaning with  clean funtion given by tweeteval  and then vectorizing with countvectorizer and bigrams for Offensive dataset\n",
        "vectorizer = CountVectorizer(stop_words=\"english\",\n",
        "                             preprocessor=preprocess,\n",
        "                             ngram_range=(1, 2))\n",
        "\n",
        "training_features = vectorizer.fit_transform(off_train_val[\"tweet_text\"])    \n",
        "test_features = vectorizer.transform(off_combo_test[\"tweet_text\"])\n",
        "\n",
        "# Training\n",
        "model = LinearSVC()\n",
        "model.fit(training_features, off_train_val[\"offensive\"])\n",
        "y_pred = model.predict(test_features)\n",
        "\n",
        "# Evaluation\n",
        "score = f1_score(off_combo_test[\"offensive\"], y_pred,average='macro')\n",
        "\n",
        "print(\"F1 score on the offensive language dataset: {:.2f}\".format(score*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score on the offensive language dataset: 71.84\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNpxsxNX2oaV",
        "outputId": "b7454852-ed2d-4b9b-df06-889b519fe163"
      },
      "source": [
        "# cleaning with clean funtion given by tweeteval  and then vectorizing with TF-IDF and bigrams for Offensive dataset\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\",\n",
        "                             preprocessor=preprocess,\n",
        "                             ngram_range=(1, 2))\n",
        "\n",
        "training_features = vectorizer.fit_transform(off_train_val[\"tweet_text\"])    \n",
        "test_features = vectorizer.transform(off_combo_test[\"tweet_text\"])\n",
        "\n",
        "# Training\n",
        "model = LinearSVC()\n",
        "model.fit(training_features, off_train_val[\"offensive\"])\n",
        "y_pred = model.predict(test_features)\n",
        "\n",
        "# Evaluation\n",
        "score = f1_score(off_combo_test[\"offensive\"], y_pred,average='macro')\n",
        "\n",
        "print(\"F1 score on the offensive language dataset: {:.2f}\".format(score*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score on the offensive language dataset: 73.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJeqJn7oIFLZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}